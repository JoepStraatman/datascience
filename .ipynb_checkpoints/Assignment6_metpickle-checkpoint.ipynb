{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Assignment 4\n",
    "\n",
    "## Notebook made by  \n",
    "\n",
    "|** Name** | **Student id** | **email**|\n",
    "|:- |:-|:-|\n",
    "|. | | |\n",
    "|  | |. |\n",
    "\n",
    "### Pledge (taken from [Coursera's Honor Code](https://www.coursera.org/about/terms/honorcode) )\n",
    "\n",
    "\n",
    "\n",
    "Put here a selfie with your photo where you hold a signed paper with the following text: (if this is team work, put two selfies here). The link must be to some place on the web, not to a local file. \n",
    "\n",
    "> My answers to homework, quizzes and exams will be my own work (except for assignments that explicitly permit collaboration).\n",
    "\n",
    ">I will not make solutions to homework, quizzes or exams available to anyone else. This includes both solutions written by me, as well as any official solutions provided by the course staff.\n",
    "\n",
    ">I will not engage in any other activities that will dishonestly improve my results or dishonestly improve/hurt the results of others.\n",
    "\n",
    "<img src='link to your selfie'/>\n",
    "\n",
    "### Note\n",
    "* **Assignments without the selfies or completely filled in information will not be graded and receive 0 points.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# P1 Parsing wikipedia\n",
    "\n",
    "#### Hints: \n",
    "1. Gebruik de code uit `ParseWikipediaDump.ipynb` in de NoteBook folder.\n",
    "2. Ontwikkel voor opgave 2:\n",
    "    * eerst een versie met alleen pageid, titel en aantal woorden\n",
    "    * ontwikkel de \"sectie-extractor\" apart op een stukje test file\n",
    "    * voeg de sectie-extractor dan in je eerdere code in.\n",
    "\n",
    "\n",
    "#### Opgaven\n",
    "1. Haal de laatste NL wikipedia dump op van <https://dumps.wikimedia.org/nlwiki/20160111/>. **update: dit is van een eerder jaar. Verwijder de datum uit de URL, kijke wat je krijgt,  en klik op de laatste datum. Wij werken in 2017 met <https://dumps.wikimedia.org/nlwiki/20170301/>**  Je hebt de file <https://dumps.wikimedia.org/nlwiki/20160111/nlwiki-20160111-pages-articles.xml.bz2> nodig, maar kunt als je dat wat groot vindt eerst beginnen met <https://dumps.wikimedia.org/nlwiki/20160111/nlwiki-20160111-pages-articles1.xml.bz2>\n",
    "2. We gaan een dataframe maken met informatie over **elke** pagina in de NL wikipedia. Dit dataframe bevat voor elke pagina de volgende 4 kolommen:\n",
    "    1. page id (dit wordt de index)\n",
    "    2. titel van de pagina\n",
    "    3. het aantal secties  (dus geen subsecties en dergelijke)\n",
    "    4. een lijst met alle namen van de secties.\n",
    "    5. het aantal woorden op de pagina (geef duidelijk in woorden aan wat je hieronder verstaat).\n",
    "2. Gebuik `%time` om te timen hoe lang je programma er over doet. Zorg dat dit duidelijk in je notebook staat. Laat ook zien hoe groot je bestand is. \n",
    "    * aantal regels, aantal kolommen, aantal cellen met missende data. \n",
    "3. Sla dit dataframe op als pickle bestand. Laat met een commando (denk aan magics) zien hoe groot het is in een voor mensen leesbare eenheid (dus niet in Bytes als het meerdere Mb groot is.\n",
    "3. Sla je dataframe op als csv bestand en probeer het in te lezen in Word of Openoffice en in Google spreadsheets. Rapporteer exact wat er gebeurt.\n",
    "4. Maak een plot waarin je op de x-as het aantal secties zet en op de y-as het aantal paginas met zoveel secties. Zorg dat het goed leesbaar is door eventueel een log schaal te gebruiken.\n",
    "5. Bereken het gemiddeld aantal secties, de mediaan, en de standaard deviatie, en geef de titel van (een?) pagina met het maximale aantal secties, en geef dat maximale aantal natuurlijk. \n",
    "    * Hoeveel paginas hebben geen secties? \n",
    "    * Is dat dan een missing data (`NaN`) of `0`. Wat is eigenlijk het verschil?\n",
    "6. Plot het aantal secties tegen het aantal woorden en bereken de correlatie.\n",
    "7. Wat is het gemiddeld aantal woorden per sectie op NL Wikipedia?\n",
    "    * Geef eerst een sluitende definitie in woorden van dit begrip. En programmeer die dan. Wees niet te snel blij en klaar. Dit is nog best tricky. \n",
    "8. **Sectie namen**\n",
    "    1. Tel voor alle sectie titels hoe vaak ze voorkomen. Laat de top 10 meest voorkomende secties zien. \n",
    "    2. Plot de sectietitels (x-as) tegen hun count (y-as) in loglog style en ga na of dat min of meer een rechte lijn is. \n",
    "        * Zo ja, dan is dit hoogstwaarschijnlijk een _power law_ (ook wel _Zipf_-verdeling genoemd) verdeling.\n",
    "        * Kan je verklaren waarom sectietitels powerlaw verdeeld zijn?\n",
    "\n",
    "#### Hint\n",
    "* Denk na of de waarden die je vind ergens op slaan.\n",
    "* Het is echt niet voldoende dat je het commando `df.mean()` kent. Het is echt de bedoeling dat je nadenkt.\n",
    "\n",
    "#### Voorbeeld\n",
    "Voor <https://nl.wikipedia.org/wiki/Albert_Speer> ziet dit er (op dit moment van schrijven) als volgt uit:\n",
    "1. page id is $1$\n",
    "2. titel is _Albert Speer_\n",
    "3. Aantal secties is 12\n",
    "4. Sectie titels zijn (maar dan zonder de nummering)\n",
    "```\n",
    "1\tVoor 1933\n",
    "2\tIn dienst van Hitler\n",
    "3\tSpeer de organisator\n",
    "4\tHitlers laatste bevelen aan Speer\n",
    "5\tArrestatie en gevangenisstraf\n",
    "6\tSpeer als de 'nette nazi'\n",
    "7\tNalatenschap\n",
    "8\tHet einde van de mythe\n",
    "9\tWetenswaardig\n",
    "10\tPublicaties\n",
    "11\tLiteratuur\n",
    "12\tFilm\n",
    "```\n",
    "\n",
    "\n",
    " \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from lxml import etree \n",
    "from bz2file import BZ2File\n",
    "import codecs\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_klein = '../nlwiki-20180301-pages-articles1.xml.bz2'\n",
    "file_groot = ''\n",
    "reference_extractor=r'[^=]={2}([^=]{1,})=='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 9s, sys: 2.35 s, total: 12min 11s\n",
      "Wall time: 1h 2min 30s\n"
     ]
    }
   ],
   "source": [
    "def count_references(f,inset): # ,outfile):\n",
    "    '''With f a wikipedia dump and inset a list of pageids, count for each page in the dump \\cap inset the number of references\n",
    "    on the page and return as a dict with pageid:reference_count key-value pairs.'''\n",
    "    with BZ2File(f) as xml_file:\n",
    "        context = etree.iterparse(xml_file,  tag= '{http://www.mediawiki.org/xml/export-0.10/}page')\n",
    "        pages_dict={}\n",
    "            \n",
    "        c=0\n",
    "        for _, elem in context:\n",
    "            \n",
    "                page_id = elem.findtext('{http://www.mediawiki.org/xml/export-0.10/}id')\n",
    "                try:  \n",
    "                    page_id= int(page_id)\n",
    "                    if  page_id in inset: # we only do this for pages in  inset\n",
    "                        pagetitle= elem.findtext('{http://www.mediawiki.org/xml/export-0.10/}title')\n",
    "                        pagetext=elem.findtext('{http://www.mediawiki.org/xml/export-0.10/}revision/{http://www.mediawiki.org/xml/export-0.10/}text')\n",
    "                        \n",
    "                        abc = len([x for x in word_tokenize(pagetext) if len(x) > 1])\n",
    "                        refs= (re.findall(reference_extractor, pagetext))\n",
    "                        ref_count= sum ( 1 for _ in refs)\n",
    "                        \n",
    "                        pages_dict[page_id] = [pagetitle, ref_count, refs, abc]\n",
    "                        #if page_id  and ref_count: # just store those with a count > 0\n",
    "                        ##pages_references_dict[page_id]= ref_count \n",
    "                        # use this when you want to write away the results to file\n",
    "                        #f.write(str(page_id)+';'+str(ref_count)+'\\n')\n",
    "                except:\n",
    "                    True\n",
    "                \n",
    "                # now get rid of the element and also delete all its ancestors   \n",
    "                # from http://stackoverflow.com/questions/12160418/why-is-lxml-etree-iterparse-eating-up-all-my-memory\n",
    "                elem.clear()\n",
    "                # Also eliminate now-empty references from the root node to elem\n",
    "                for ancestor in elem.xpath('ancestor-or-self::*'):\n",
    "                    while ancestor.getprevious() is not None:\n",
    "                        del ancestor.getparent()[0]    \n",
    "                # for debugging and seeing how far the code is already\n",
    "                c+=1\n",
    "#                 if c% 10**3==0:\n",
    "#                      print(c)  #    \n",
    "#                      #break #\n",
    "        del context\n",
    "    #dfpagetitel= pd.DataFrame.from_dict(dict_pagetitel,orient='index')\n",
    "    #dfpagesectie=pd.DataFrame.from_dict(dict_pagesecties,orient='index')\n",
    "    #dflijstsectie= pd.DataFrame.from_dict(dict_lijstsecties,orient='index')\n",
    "    df = pd.DataFrame.from_dict(pages_dict,orient='index')\n",
    "    #pandadf= pd.concat([dfpagetitel,dfpagesectie,dflijstsectie,dfwordcount],axis=1)\n",
    "    df.columns=['Pagetitel', 'Sectieaantal','Sectienaam','Woordaantal']\n",
    "    return df\n",
    "%time dfp = count_references(file_klein, set(range(1000000)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'woordensectie'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2441\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2442\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2443\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'woordensectie'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9d82d05c7e12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdfp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'woordensecties'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWoordaantal\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSectieaantal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdfp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'woordensectie'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1962\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1964\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1969\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1970\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1971\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1973\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1643\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1645\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3589\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3590\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3591\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3592\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2442\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2443\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2444\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2446\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'woordensectie'"
     ]
    }
   ],
   "source": [
    "dfp['woordensecties'] = dfp.Woordaantal/dfp.Sectieaantal\n",
    "dfp = dfp.replace([np.inf, -np.inf], 0)\n",
    "dfp['woordensectie'].mean()\n",
    "dfp\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pagetext = '== Voor 1933 == Het gezin waarin Speer werd geboren, was wat ze in het Duits noemen ''grossbürgerlich''; in Nederlandse termen gegoede [[burgerij]] of bourgeoisie. Zijn vader en grootvader waren beiden architecten. Om financiële redenen studeerde Speer aanvankelijk in [[Karlsruhe (stad)|Karlsruhe]]. Van de lente van [[1924]] tot de zomer van [[1925]] zette hij zijn studie vervolgens voort aan de technische hogeschool van [[München]]. In de herfst van 1925 verhuisde hij naar [[Berlijn (hoofdbetekenis)|Berlijn]] en probeerde hij vergeefs aan de [[Technische Universiteit Berlijn|Technische Hogeschool]] in [[Berlin-Charlottenburg|Berlijn-Charlottenburg]] in het seminarie van [[Hans Poelzig]] toegelaten te worden. In [[1926]] ontving [[Heinrich Tessenow]], een architect van de behoudende school met een zeer bescheiden en niet megalomane stijl, een leerstoel. Speer werd in dat jaar één van zijn studenten. Na zijn diploma te hebben behaald in [[1927]] bleef Speer nog meerdere jaren, als Tessenows assistent, aan de hogeschool verbonden. == Architect in dienst van Hitler == '.split(' ')\n",
    "\n",
    "re.findall(reference_extractor, pagetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Voorbeeld plot code  # Haal dit uit je antwoorden\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "df = pd.read_csv('http://www.stat.ubc.ca/~jenny/notOcto/STAT545A/examples/gapminder/data/gapminderDataFiveYear.txt', sep='\\t')\n",
    "df2007 = df[df.year==2007]\n",
    "df1952 = df[df.year==1952]\n",
    "print (df.head(2))\n",
    "\n",
    "df.plot(x='year', y='pop');\n",
    "\n",
    "df.plot.scatter(x='gdpPercap', y='lifeExp', logx=True); # upgrade to pandas 17.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = 'fdsfsd'\n",
    "h = 'sadsa'\n",
    "l = 321\n",
    "k = ['dsf', 'dsfsfds']\n",
    "lel = {}\n",
    "lel[2] = [t,h,l,k]\n",
    "lel[1] = [t,h,l,k]\n",
    "dfwordcount = pd.DataFrame.from_dict(lel,orient='index')\n",
    "dfwordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dfp\n",
    "picklebestand = dfp.to_pickle('pikkel.pkl')\n",
    "!ls -lh 'pikkel.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfp.to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
